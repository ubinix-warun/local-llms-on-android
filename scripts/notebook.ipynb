{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "https://github.com/ubinix-warun/local-llms-on-android/blob/main/scripts/notebook.ipynb",
      "authorship_tag": "ABX9TyPA8yUNyQnr1HVnCbKVACEE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ubinix-warun/local-llms-on-android/blob/main/scripts/notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ubinix-warun/local-llms-on-android.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uvt53A-N9O87",
        "outputId": "3c0fbc20-23ef-4197-cd04-f6f0a045b80d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'local-llms-on-android'...\n",
            "remote: Enumerating objects: 635, done.\u001b[K\n",
            "remote: Counting objects: 100% (393/393), done.\u001b[K\n",
            "remote: Compressing objects: 100% (255/255), done.\u001b[K\n",
            "remote: Total 635 (delta 177), reused 255 (delta 88), pack-reused 242 (from 1)\u001b[K\n",
            "Receiving objects: 100% (635/635), 24.92 MiB | 23.37 MiB/s, done.\n",
            "Resolving deltas: 100% (252/252), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd local-llms-on-android/scripts && \\\n",
        "  pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8mijLs6w-QAJ",
        "outputId": "575f7578-9a5b-4b61-f7d7-d512632217f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optimum@ git+https://github.com/huggingface/optimum.git (from -r requirements.txt (line 12))\n",
            "  Cloning https://github.com/huggingface/optimum.git to /tmp/pip-install-skr0h0m2/optimum_2149ebf7251c4f91ab2da3e2037b9af6\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/optimum.git /tmp/pip-install-skr0h0m2/optimum_2149ebf7251c4f91ab2da3e2037b9af6\n",
            "  Resolved https://github.com/huggingface/optimum.git to commit 0227a1ce9652b1b02da5a510bf513c585608f8c2\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: onnx in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 2)) (1.20.0)\n",
            "Requirement already satisfied: onnxruntime in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 3)) (1.23.2)\n",
            "Collecting onnxruntime-gpu (from -r requirements.txt (line 4))\n",
            "  Downloading onnxruntime_gpu-1.23.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.4 kB)\n",
            "Collecting onnx_graphsurgeon (from -r requirements.txt (line 5))\n",
            "  Downloading onnx_graphsurgeon-0.5.8-py2.py3-none-any.whl.metadata (8.2 kB)\n",
            "Collecting onnxconverter-common (from -r requirements.txt (line 6))\n",
            "  Downloading onnxconverter_common-1.16.0-py2.py3-none-any.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 7)) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 10)) (0.21.4)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 11)) (4.55.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 15)) (2.0.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 16)) (1.14.0)\n",
            "Requirement already satisfied: mpmath in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 17)) (1.3.0)\n",
            "Requirement already satisfied: protobuf>=4.25.1 in /usr/local/lib/python3.12/dist-packages (from onnx->-r requirements.txt (line 2)) (6.33.1)\n",
            "Requirement already satisfied: typing_extensions>=4.7.1 in /usr/local/lib/python3.12/dist-packages (from onnx->-r requirements.txt (line 2)) (4.15.0)\n",
            "Requirement already satisfied: ml_dtypes>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from onnx->-r requirements.txt (line 2)) (0.5.4)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.12/dist-packages (from onnxruntime->-r requirements.txt (line 3)) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime->-r requirements.txt (line 3)) (25.9.23)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from onnxruntime->-r requirements.txt (line 3)) (25.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.12/dist-packages (from tokenizers->-r requirements.txt (line 10)) (0.36.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers->-r requirements.txt (line 11)) (3.20.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers->-r requirements.txt (line 11)) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers->-r requirements.txt (line 11)) (2.32.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers->-r requirements.txt (line 11)) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers->-r requirements.txt (line 11)) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11 in /usr/local/lib/python3.12/dist-packages (from optimum@ git+https://github.com/huggingface/optimum.git->-r requirements.txt (line 12)) (2.9.0+cpu)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->-r requirements.txt (line 10)) (2025.12.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->-r requirements.txt (line 10)) (1.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->optimum@ git+https://github.com/huggingface/optimum.git->-r requirements.txt (line 12)) (75.2.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->optimum@ git+https://github.com/huggingface/optimum.git->-r requirements.txt (line 12)) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->optimum@ git+https://github.com/huggingface/optimum.git->-r requirements.txt (line 12)) (3.1.6)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.12/dist-packages (from coloredlogs->onnxruntime->-r requirements.txt (line 3)) (10.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers->-r requirements.txt (line 11)) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers->-r requirements.txt (line 11)) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers->-r requirements.txt (line 11)) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers->-r requirements.txt (line 11)) (2025.11.12)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11->optimum@ git+https://github.com/huggingface/optimum.git->-r requirements.txt (line 12)) (3.0.3)\n",
            "Downloading onnxruntime_gpu-1.23.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (300.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.5/300.5 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnx_graphsurgeon-0.5.8-py2.py3-none-any.whl (57 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.9/57.9 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxconverter_common-1.16.0-py2.py3-none-any.whl (89 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.5/89.5 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: optimum\n",
            "  Building wheel for optimum (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for optimum: filename=optimum-2.1.0.dev0-py3-none-any.whl size=162229 sha256=3528a52fb03c208cc98719a3729ccc588ab692ed89b7129ab64eff694ed8eff9\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-3ni54woq/wheels/88/3d/4e/082f1a517240fa457dbd77485e6d08a64f31cbcc0ca7526605\n",
            "Successfully built optimum\n",
            "Installing collected packages: onnxruntime-gpu, onnxconverter-common, onnx_graphsurgeon, optimum\n",
            "  Attempting uninstall: optimum\n",
            "    Found existing installation: optimum 2.0.0\n",
            "    Uninstalling optimum-2.0.0:\n",
            "      Successfully uninstalled optimum-2.0.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "optimum-onnx 0.0.3 requires optimum~=2.0.0, but you have optimum 2.1.0.dev0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed onnx_graphsurgeon-0.5.8 onnxconverter-common-1.16.0 onnxruntime-gpu-1.23.2 optimum-2.1.0.dev0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://huggingface.co/onnx-community/Qwen3-0.6B-ONNX/resolve/main/onnx/model_q4.onnx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YGxi4JEt95xh",
        "outputId": "d48b5ffa-57e7-4c57-fec7-d12effcc845a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-12-10 02:54:27--  https://huggingface.co/onnx-community/Qwen3-0.6B-ONNX/resolve/main/onnx/model_q4.onnx\n",
            "Resolving huggingface.co (huggingface.co)... 3.165.160.12, 3.165.160.61, 3.165.160.11, ...\n",
            "Connecting to huggingface.co (huggingface.co)|3.165.160.12|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cas-bridge.xethub.hf.co/xet-bridge-us/680fffcdbe536d99d3cb19b9/c5e1a7c527e70efb93d47c9058c3bb2b5a3cc56700a07479f55aa4c1b9981a7c?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20251210%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20251210T025427Z&X-Amz-Expires=3600&X-Amz-Signature=21175c3bb9fd833b1450add6c7d2a352113ba38e557bf2f3a8ce8882e7e98a8b&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27model_q4.onnx%3B+filename%3D%22model_q4.onnx%22%3B&x-id=GetObject&Expires=1765338867&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc2NTMzODg2N319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82ODBmZmZjZGJlNTM2ZDk5ZDNjYjE5YjkvYzVlMWE3YzUyN2U3MGVmYjkzZDQ3YzkwNThjM2JiMmI1YTNjYzU2NzAwYTA3NDc5ZjU1YWE0YzFiOTk4MWE3YyoifV19&Signature=ks2vCDmt3YVHvlRlwQBhJeGnEF6yxdk%7EHUNtI-BAH4o55Q44BtFoGVXIdmKiH070-A4JtVanRLzDbbXDQOYM-517bCZXMqPgaLGVwuTu9kgvkRQugWDRfQWqXo9yvAoP-gVFT%7EysRR6Ob3L27ohlZ8cEAsFtaNuQywDFiqI1bjIHJHNW7oN8W4MQEXk6o0IwVvNBerKdP%7EG4QkbE6l7%7ERcXEQfdBF3iEGPef%7ERUGSqaklgJMpqPjIyLbTuoe8IEMgNTShQ14xRDLb1ePDzvQl8gnPAw6p9LO6C3jGudIAPn9kfyVnRBAKlTQZlQOu2so2SNDdEaAcblyYobU3uYt8w__&Key-Pair-Id=K2L8F4GPSG1IFC [following]\n",
            "--2025-12-10 02:54:27--  https://cas-bridge.xethub.hf.co/xet-bridge-us/680fffcdbe536d99d3cb19b9/c5e1a7c527e70efb93d47c9058c3bb2b5a3cc56700a07479f55aa4c1b9981a7c?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20251210%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20251210T025427Z&X-Amz-Expires=3600&X-Amz-Signature=21175c3bb9fd833b1450add6c7d2a352113ba38e557bf2f3a8ce8882e7e98a8b&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27model_q4.onnx%3B+filename%3D%22model_q4.onnx%22%3B&x-id=GetObject&Expires=1765338867&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc2NTMzODg2N319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82ODBmZmZjZGJlNTM2ZDk5ZDNjYjE5YjkvYzVlMWE3YzUyN2U3MGVmYjkzZDQ3YzkwNThjM2JiMmI1YTNjYzU2NzAwYTA3NDc5ZjU1YWE0YzFiOTk4MWE3YyoifV19&Signature=ks2vCDmt3YVHvlRlwQBhJeGnEF6yxdk%7EHUNtI-BAH4o55Q44BtFoGVXIdmKiH070-A4JtVanRLzDbbXDQOYM-517bCZXMqPgaLGVwuTu9kgvkRQugWDRfQWqXo9yvAoP-gVFT%7EysRR6Ob3L27ohlZ8cEAsFtaNuQywDFiqI1bjIHJHNW7oN8W4MQEXk6o0IwVvNBerKdP%7EG4QkbE6l7%7ERcXEQfdBF3iEGPef%7ERUGSqaklgJMpqPjIyLbTuoe8IEMgNTShQ14xRDLb1ePDzvQl8gnPAw6p9LO6C3jGudIAPn9kfyVnRBAKlTQZlQOu2so2SNDdEaAcblyYobU3uYt8w__&Key-Pair-Id=K2L8F4GPSG1IFC\n",
            "Resolving cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)... 18.238.217.64, 18.238.217.126, 18.238.217.63, ...\n",
            "Connecting to cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)|18.238.217.64|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 919096585 (877M)\n",
            "Saving to: ‘model_q4.onnx’\n",
            "\n",
            "model_q4.onnx       100%[===================>] 876.52M  92.0MB/s    in 5.3s    \n",
            "\n",
            "2025-12-10 02:54:32 (165 MB/s) - ‘model_q4.onnx’ saved [919096585/919096585]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://huggingface.co/onnx-community/Qwen3-0.6B-ONNX/resolve/main/tokenizer.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mps73Q3F_z1Z",
        "outputId": "d8c85d84-8d41-4f61-d723-261c9358a21a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-12-10 02:54:34--  https://huggingface.co/onnx-community/Qwen3-0.6B-ONNX/resolve/main/tokenizer.json\n",
            "Resolving huggingface.co (huggingface.co)... 3.165.160.12, 3.165.160.59, 3.165.160.11, ...\n",
            "Connecting to huggingface.co (huggingface.co)|3.165.160.12|:443... connected.\n",
            "HTTP request sent, awaiting response... 307 Temporary Redirect\n",
            "Location: /api/resolve-cache/models/onnx-community/Qwen3-0.6B-ONNX/b1ece21c06dfce3839272e86b7fa12a985d97a7a/tokenizer.json?%2Fonnx-community%2FQwen3-0.6B-ONNX%2Fresolve%2Fmain%2Ftokenizer.json=&etag=%226c0f4b0f97c638fb9b293aaa8bf99a159e650ed0%22 [following]\n",
            "--2025-12-10 02:54:34--  https://huggingface.co/api/resolve-cache/models/onnx-community/Qwen3-0.6B-ONNX/b1ece21c06dfce3839272e86b7fa12a985d97a7a/tokenizer.json?%2Fonnx-community%2FQwen3-0.6B-ONNX%2Fresolve%2Fmain%2Ftokenizer.json=&etag=%226c0f4b0f97c638fb9b293aaa8bf99a159e650ed0%22\n",
            "Reusing existing connection to huggingface.co:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9117040 (8.7M) [text/plain]\n",
            "Saving to: ‘tokenizer.json’\n",
            "\n",
            "tokenizer.json      100%[===================>]   8.69M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2025-12-10 02:54:34 (153 MB/s) - ‘tokenizer.json’ saved [9117040/9117040]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S7rUDsV67uXr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91d6cd24-51bc-438d-a6b0-9539cde3c137"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Generated Text ===\n",
            " What is the capital of France? What is the capital of Italy? What is the capital of Spain? What is the capital of Russia? What is the capital of Brazil? What is the capital of China? What is the capital of Japan? What\n"
          ]
        }
      ],
      "source": [
        "!cd local-llms-on-android/scripts && \\\n",
        "  python infer_onnx_with_kv_cache.py \\\n",
        "    --model qwen3 \\\n",
        "    --model_path /content/model_q4.onnx \\\n",
        "    --tokenizer_path /content/tokenizer.json \\\n",
        "    --prompt \"What is the capital of Germany?\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optimum[onnxruntime]"
      ],
      "metadata": {
        "id": "me3yailABJ7H",
        "outputId": "ded5c419-4160-437c-91f6-970a742287e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optimum[onnxruntime]\n",
            "  Downloading optimum-2.1.0-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: transformers>=4.29 in /usr/local/lib/python3.12/dist-packages (from optimum[onnxruntime]) (4.57.3)\n",
            "Requirement already satisfied: torch>=1.11 in /usr/local/lib/python3.12/dist-packages (from optimum[onnxruntime]) (2.9.0+cpu)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from optimum[onnxruntime]) (25.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from optimum[onnxruntime]) (2.0.2)\n",
            "Requirement already satisfied: huggingface_hub>=0.8.0 in /usr/local/lib/python3.12/dist-packages (from optimum[onnxruntime]) (0.36.0)\n",
            "Collecting optimum-onnx[onnxruntime] (from optimum[onnxruntime])\n",
            "  Downloading optimum_onnx-0.0.3-py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.8.0->optimum[onnxruntime]) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.8.0->optimum[onnxruntime]) (2025.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.8.0->optimum[onnxruntime]) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.8.0->optimum[onnxruntime]) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.8.0->optimum[onnxruntime]) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.8.0->optimum[onnxruntime]) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.8.0->optimum[onnxruntime]) (1.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->optimum[onnxruntime]) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->optimum[onnxruntime]) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->optimum[onnxruntime]) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->optimum[onnxruntime]) (3.1.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.29->optimum[onnxruntime]) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.29->optimum[onnxruntime]) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.29->optimum[onnxruntime]) (0.7.0)\n",
            "INFO: pip is looking at multiple versions of optimum-onnx[onnxruntime] to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading optimum_onnx-0.0.2-py3-none-any.whl.metadata (4.6 kB)\n",
            "  Downloading optimum_onnx-0.0.1-py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting optimum[onnxruntime]\n",
            "  Downloading optimum-2.0.0-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting transformers>=4.29 (from optimum[onnxruntime])\n",
            "  Downloading transformers-4.55.4-py3-none-any.whl.metadata (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting onnx (from optimum-onnx[onnxruntime]; extra == \"onnxruntime\"->optimum[onnxruntime])\n",
            "  Downloading onnx-1.20.0-cp312-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
            "Collecting onnxruntime>=1.18.0 (from optimum-onnx[onnxruntime]; extra == \"onnxruntime\"->optimum[onnxruntime])\n",
            "  Downloading onnxruntime-1.23.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\n",
            "Collecting tokenizers<0.22,>=0.21 (from transformers>=4.29->optimum[onnxruntime])\n",
            "  Downloading tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Collecting coloredlogs (from onnxruntime>=1.18.0->optimum-onnx[onnxruntime]; extra == \"onnxruntime\"->optimum[onnxruntime])\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.18.0->optimum-onnx[onnxruntime]; extra == \"onnxruntime\"->optimum[onnxruntime]) (25.9.23)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.18.0->optimum-onnx[onnxruntime]; extra == \"onnxruntime\"->optimum[onnxruntime]) (5.29.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11->optimum[onnxruntime]) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11->optimum[onnxruntime]) (3.0.3)\n",
            "Requirement already satisfied: ml_dtypes>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from onnx->optimum-onnx[onnxruntime]; extra == \"onnxruntime\"->optimum[onnxruntime]) (0.5.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.8.0->optimum[onnxruntime]) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.8.0->optimum[onnxruntime]) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.8.0->optimum[onnxruntime]) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.8.0->optimum[onnxruntime]) (2025.11.12)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.18.0->optimum-onnx[onnxruntime]; extra == \"onnxruntime\"->optimum[onnxruntime])\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Downloading optimum-2.0.0-py3-none-any.whl (162 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.3/162.3 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading transformers-4.55.4-py3-none-any.whl (11.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m93.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.23.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (17.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m68.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m85.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnx-1.20.0-cp312-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (18.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.1/18.1 MB\u001b[0m \u001b[31m75.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading optimum_onnx-0.0.3-py3-none-any.whl (192 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m192.3/192.3 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: humanfriendly, onnx, coloredlogs, tokenizers, onnxruntime, transformers, optimum, optimum-onnx\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.22.1\n",
            "    Uninstalling tokenizers-0.22.1:\n",
            "      Successfully uninstalled tokenizers-0.22.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.57.3\n",
            "    Uninstalling transformers-4.57.3:\n",
            "      Successfully uninstalled transformers-4.57.3\n",
            "Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 onnx-1.20.0 onnxruntime-1.23.2 optimum-2.0.0 optimum-onnx-0.0.3 tokenizers-0.21.4 transformers-4.55.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!optimum-cli export onnx \\\n",
        "  --model Qwen/Qwen3-4B-Instruct-2507 qwen3-4b-instruct-2507"
      ],
      "metadata": {
        "id": "EqZffQQTBRZZ",
        "outputId": "790bdc15-2520-4138-b828-2623119d6c05",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jax/_src/cloud_tpu_init.py:86: UserWarning: Transparent hugepages are not enabled. TPU runtime startup and shutdown time should be significantly improved on TPU v5e and newer. If not already set, you may need to enable transparent hugepages in your VM image (sudo sh -c \"echo always > /sys/kernel/mm/transparent_hugepage/enabled\")\n",
            "  warnings.warn(\n",
            "config.json: 100% 727/727 [00:00<00:00, 9.93MB/s]\n",
            "model.safetensors.index.json: 32.8kB [00:00, 152MB/s]\n",
            "Fetching 3 files:   0% 0/3 [00:00<?, ?it/s]\n",
            "model-00002-of-00003.safetensors:   0% 0.00/3.99G [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "model-00003-of-00003.safetensors:   0% 0.00/99.6M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:   0% 0.00/3.96G [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:   0% 880k/3.99G [00:01<1:44:20, 637kB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:   0% 2.56M/3.99G [00:01<36:40, 1.81MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:   0% 630k/3.96G [00:01<3:17:18, 334kB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:   2% 69.6M/3.99G [00:02<01:36, 40.4MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:   3% 137M/3.99G [00:02<00:46, 82.5MB/s] \u001b[A\n",
            "\n",
            "model-00003-of-00003.safetensors:  33% 32.6M/99.6M [00:02<00:05, 11.3MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:   7% 271M/3.99G [00:02<00:20, 184MB/s] \u001b[A\n",
            "model-00002-of-00003.safetensors:   8% 338M/3.99G [00:02<00:15, 236MB/s]\u001b[A\n",
            "\n",
            "model-00003-of-00003.safetensors: 100% 99.6M/99.6M [00:03<00:00, 33.2MB/s]\n",
            "\n",
            "model-00002-of-00003.safetensors:  12% 472M/3.99G [00:03<00:09, 373MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  15% 606M/3.99G [00:03<00:07, 457MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  17% 673M/3.99G [00:03<00:08, 379MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  20% 807M/3.99G [00:03<00:07, 408MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:   2% 67.7M/3.96G [00:04<03:24, 19.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  24% 941M/3.99G [00:05<00:17, 174MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:   3% 135M/3.96G [00:06<02:57, 21.6MB/s] \u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  25% 1.01G/3.99G [00:06<00:27, 109MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  27% 1.08G/3.99G [00:07<00:24, 119MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:   7% 269M/3.96G [00:07<01:19, 46.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  29% 1.14G/3.99G [00:08<00:30, 94.3MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:   8% 336M/3.96G [00:10<01:34, 38.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  30% 1.21G/3.99G [00:10<00:41, 67.6MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  32% 1.28G/3.99G [00:12<00:50, 53.2MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  12% 470M/3.96G [00:12<01:13, 47.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  34% 1.34G/3.99G [00:12<00:37, 71.4MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  14% 537M/3.96G [00:12<01:00, 56.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  35% 1.41G/3.99G [00:15<00:55, 46.2MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  15% 604M/3.96G [00:15<01:15, 44.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  37% 1.48G/3.99G [00:15<00:40, 62.3MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  17% 671M/3.96G [00:15<01:02, 52.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  39% 1.55G/3.99G [00:16<00:40, 60.8MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  19% 738M/3.96G [00:17<01:00, 53.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  40% 1.61G/3.99G [00:19<00:56, 41.9MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  20% 805M/3.96G [00:19<01:12, 43.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  22% 873M/3.96G [00:19<00:55, 55.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  44% 1.75G/3.99G [00:20<00:35, 62.3MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  24% 940M/3.96G [00:21<00:56, 53.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  46% 1.82G/3.99G [00:21<00:34, 62.1MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  47% 1.88G/3.99G [00:23<00:42, 49.2MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  25% 1.01G/3.96G [00:23<01:09, 42.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  49% 1.95G/3.99G [00:23<00:31, 64.9MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  29% 1.14G/3.96G [00:24<00:47, 59.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  51% 2.02G/3.99G [00:24<00:32, 61.3MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  52% 2.08G/3.99G [00:27<00:44, 42.3MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  31% 1.21G/3.96G [00:27<01:05, 42.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  54% 2.15G/3.99G [00:27<00:31, 58.0MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  34% 1.34G/3.96G [00:27<00:37, 69.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  56% 2.22G/3.99G [00:28<00:25, 69.9MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  36% 1.41G/3.96G [00:28<00:34, 75.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  57% 2.28G/3.99G [00:29<00:24, 69.2MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  37% 1.48G/3.96G [00:29<00:36, 67.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  59% 2.35G/3.99G [00:30<00:26, 62.9MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  39% 1.54G/3.96G [00:31<00:40, 59.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  61% 2.42G/3.99G [00:31<00:26, 59.1MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  41% 1.61G/3.96G [00:32<00:40, 57.8MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  62% 2.49G/3.99G [00:33<00:26, 57.4MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  42% 1.68G/3.96G [00:33<00:39, 57.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  64% 2.55G/3.99G [00:34<00:25, 56.6MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  44% 1.75G/3.96G [00:35<00:39, 55.8MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  66% 2.62G/3.99G [00:35<00:24, 55.6MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  46% 1.81G/3.96G [00:36<00:39, 54.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  67% 2.69G/3.99G [00:36<00:23, 56.3MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  48% 1.88G/3.96G [00:37<00:38, 53.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  69% 2.75G/3.99G [00:38<00:22, 54.8MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  49% 1.95G/3.96G [00:38<00:37, 52.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  71% 2.82G/3.99G [00:39<00:21, 55.4MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  51% 2.02G/3.96G [00:40<00:37, 51.8MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  72% 2.89G/3.99G [00:40<00:20, 54.0MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  53% 2.08G/3.96G [00:41<00:35, 52.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  74% 2.95G/3.99G [00:41<00:19, 53.4MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  54% 2.15G/3.96G [00:42<00:35, 51.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  76% 3.02G/3.99G [00:42<00:17, 54.1MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  56% 2.22G/3.96G [00:44<00:32, 52.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  77% 3.09G/3.99G [00:44<00:16, 54.0MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  58% 2.28G/3.96G [00:45<00:30, 54.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  79% 3.15G/3.99G [00:45<00:15, 52.9MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  59% 2.35G/3.96G [00:46<00:30, 52.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  81% 3.22G/3.99G [00:46<00:14, 52.5MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  61% 2.42G/3.96G [00:47<00:28, 53.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  82% 3.29G/3.99G [00:48<00:13, 53.6MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  63% 2.48G/3.96G [00:49<00:27, 53.8MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  84% 3.36G/3.99G [00:49<00:11, 54.7MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  64% 2.55G/3.96G [00:50<00:26, 53.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  86% 3.42G/3.99G [00:50<00:10, 54.1MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  66% 2.62G/3.96G [00:51<00:25, 52.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  88% 3.49G/3.99G [00:51<00:09, 52.3MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  68% 2.69G/3.96G [00:52<00:24, 52.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  89% 3.56G/3.99G [00:53<00:08, 51.6MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  70% 2.75G/3.96G [00:54<00:22, 52.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  91% 3.62G/3.99G [00:54<00:06, 52.3MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  92% 3.65G/3.99G [00:55<00:06, 52.8MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  71% 2.82G/3.96G [00:55<00:21, 52.8MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  93% 3.72G/3.99G [00:56<00:05, 52.1MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  73% 2.89G/3.96G [00:56<00:20, 53.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  95% 3.79G/3.99G [00:57<00:03, 52.9MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  75% 2.95G/3.96G [00:57<00:18, 53.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  97% 3.85G/3.99G [00:58<00:02, 52.6MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  76% 3.02G/3.96G [00:59<00:17, 53.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  98% 3.92G/3.99G [01:00<00:01, 53.3MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  78% 3.09G/3.96G [01:00<00:16, 53.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors: 100% 3.99G/3.99G [01:01<00:00, 65.1MB/s]\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  80% 3.15G/3.96G [01:01<00:14, 55.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  81% 3.22G/3.96G [01:02<00:10, 67.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  83% 3.29G/3.96G [01:02<00:09, 73.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  85% 3.36G/3.96G [01:03<00:07, 80.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  86% 3.42G/3.96G [01:04<00:06, 85.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  88% 3.49G/3.96G [01:04<00:05, 92.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  90% 3.56G/3.96G [01:05<00:04, 96.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  92% 3.62G/3.96G [01:05<00:03, 101MB/s] \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  93% 3.69G/3.96G [01:06<00:02, 102MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  95% 3.76G/3.96G [01:07<00:01, 105MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  97% 3.82G/3.96G [01:07<00:01, 107MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  98% 3.89G/3.96G [01:08<00:00, 107MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors: 100% 3.96G/3.96G [01:08<00:00, 57.4MB/s]\n",
            "Fetching 3 files: 100% 3/3 [01:09<00:00, 23.16s/it]\n",
            "Loading checkpoint shards: 100% 3/3 [00:03<00:00,  1.12s/it]\n",
            "generation_config.json: 100% 238/238 [00:00<00:00, 3.48MB/s]\n",
            "tokenizer_config.json: 9.38kB [00:00, 51.7MB/s]\n",
            "vocab.json: 2.78MB [00:00, 5.52MB/s]\n",
            "merges.txt: 1.67MB [00:00, 35.8MB/s]\n",
            "tokenizer.json: 100% 11.4M/11.4M [00:00<00:00, 28.4MB/s]\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/cache_utils.py:108: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if self.keys is None or self.keys.numel() == 0:\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/masking_utils.py:190: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if (padding_length := kv_length + kv_offset - attention_mask.shape[-1]) > 0:\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/masking_utils.py:218: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if padding_mask is not None and padding_mask.shape[-1] > kv_length:\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/integrations/sdpa_attention.py:82: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  is_causal = query.shape[2] > 1 and attention_mask is None and getattr(module, \"is_causal\", True)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/onnx/_internal/torchscript_exporter/symbolic_opset9.py:5353: UserWarning: Exporting aten::index operator of advanced indexing in opset 18 is achieved by combination of multiple ONNX operators, including Reshape, Transpose, Concat, and Gather. If indices include negative values, the exported graph will produce incorrect results.\n",
            "  warnings.warn(\n",
            "Found different candidate ONNX initializers (likely duplicate) for the tied weights:\n",
            "\tlm_head.weight: {'onnx::MatMul_12758'}\n",
            "\tmodel.embed_tokens.weight: {'model.embed_tokens.weight'}\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -alh qwen3-4b-instruct-2507/*.onnx*"
      ],
      "metadata": {
        "id": "pWPwFdpWZ0_I",
        "outputId": "cd4951c0-32e5-40aa-fef9-718e788da911",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-rw-r--r-- 1 root root 1.8M Dec 10 03:32 qwen3-4b-instruct-2507/model.onnx\n",
            "-rw-r--r-- 1 root root  17G Dec 10 03:32 qwen3-4b-instruct-2507/model.onnx_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd local-llms-on-android/scripts && \\\n",
        "  python infer_onnx_with_kv_cache.py \\\n",
        "    --model qwen3 \\\n",
        "    --model_path /content/qwen3-4b-instruct-2507/model.onnx \\\n",
        "    --tokenizer_path /content/qwen3-4b-instruct-2507/tokenizer.json \\\n",
        "    --prompt \"What is the capital of Germany?\""
      ],
      "metadata": {
        "id": "_jsSSvida3YC",
        "outputId": "d12e42d3-5b6e-4813-b386-5ccb50e39016",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;31m2025-12-10 04:02:09.501127938 [E:onnxruntime:Default, provider_bridge_ort.cc:2251 TryGetProviderInfo_CUDA] /onnxruntime_src/onnxruntime/core/session/provider_bridge_ort.cc:1844 onnxruntime::Provider& onnxruntime::ProviderLibrary::Get() [ONNXRuntimeError] : 1 : FAIL : Failed to load library libonnxruntime_providers_cuda.so with error: libcublasLt.so.12: cannot open shared object file: No such file or directory\n",
            "\u001b[m\n",
            "\u001b[0;93m2025-12-10 04:02:09.501152049 [W:onnxruntime:Default, onnxruntime_pybind_state.cc:1013 CreateExecutionProviderFactoryInstance] Failed to create CUDAExecutionProvider. Require cuDNN 9.* and CUDA 12.*. Please install all dependencies as mentioned in the GPU requirements page (https://onnxruntime.ai/docs/execution-providers/CUDA-ExecutionProvider.html#requirements), make sure they're in the PATH, and that your GPU is supported.\u001b[m\n",
            "[ERROR] Inference failed: Required inputs (['past_key_values.28.key', 'past_key_values.28.value', 'past_key_values.29.key', 'past_key_values.29.value', 'past_key_values.30.key', 'past_key_values.30.value', 'past_key_values.31.key', 'past_key_values.31.value', 'past_key_values.32.key', 'past_key_values.32.value', 'past_key_values.33.key', 'past_key_values.33.value', 'past_key_values.34.key', 'past_key_values.34.value', 'past_key_values.35.key', 'past_key_values.35.value']) are missing from input feed (['input_ids', 'attention_mask', 'position_ids', 'past_key_values.0.key', 'past_key_values.0.value', 'past_key_values.1.key', 'past_key_values.1.value', 'past_key_values.2.key', 'past_key_values.2.value', 'past_key_values.3.key', 'past_key_values.3.value', 'past_key_values.4.key', 'past_key_values.4.value', 'past_key_values.5.key', 'past_key_values.5.value', 'past_key_values.6.key', 'past_key_values.6.value', 'past_key_values.7.key', 'past_key_values.7.value', 'past_key_values.8.key', 'past_key_values.8.value', 'past_key_values.9.key', 'past_key_values.9.value', 'past_key_values.10.key', 'past_key_values.10.value', 'past_key_values.11.key', 'past_key_values.11.value', 'past_key_values.12.key', 'past_key_values.12.value', 'past_key_values.13.key', 'past_key_values.13.value', 'past_key_values.14.key', 'past_key_values.14.value', 'past_key_values.15.key', 'past_key_values.15.value', 'past_key_values.16.key', 'past_key_values.16.value', 'past_key_values.17.key', 'past_key_values.17.value', 'past_key_values.18.key', 'past_key_values.18.value', 'past_key_values.19.key', 'past_key_values.19.value', 'past_key_values.20.key', 'past_key_values.20.value', 'past_key_values.21.key', 'past_key_values.21.value', 'past_key_values.22.key', 'past_key_values.22.value', 'past_key_values.23.key', 'past_key_values.23.value', 'past_key_values.24.key', 'past_key_values.24.value', 'past_key_values.25.key', 'past_key_values.25.value', 'past_key_values.26.key', 'past_key_values.26.value', 'past_key_values.27.key', 'past_key_values.27.value']).\n",
            "\n",
            "=== Generated Text ===\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!optimum-cli export onnx \\\n",
        "  --model Qwen/Qwen2.5-0.5B-Instruct qwen2.5-0.5B-onnx/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OuVbSs0dH5Cw",
        "outputId": "e31878f3-721a-4351-aa2b-1c67e896cb8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jax/_src/cloud_tpu_init.py:86: UserWarning: Transparent hugepages are not enabled. TPU runtime startup and shutdown time should be significantly improved on TPU v5e and newer. If not already set, you may need to enable transparent hugepages in your VM image (sudo sh -c \"echo always > /sys/kernel/mm/transparent_hugepage/enabled\")\n",
            "  warnings.warn(\n",
            "config.json: 100% 659/659 [00:00<00:00, 6.69MB/s]\n",
            "model.safetensors: 100% 988M/988M [00:04<00:00, 231MB/s]\n",
            "generation_config.json: 100% 242/242 [00:00<00:00, 2.05MB/s]\n",
            "tokenizer_config.json: 7.30kB [00:00, 21.6MB/s]\n",
            "vocab.json: 2.78MB [00:00, 112MB/s]\n",
            "merges.txt: 1.67MB [00:00, 178MB/s]\n",
            "tokenizer.json: 7.03MB [00:00, 220MB/s]\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/cache_utils.py:108: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if self.keys is None or self.keys.numel() == 0:\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/masking_utils.py:190: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if (padding_length := kv_length + kv_offset - attention_mask.shape[-1]) > 0:\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/masking_utils.py:218: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if padding_mask is not None and padding_mask.shape[-1] > kv_length:\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/integrations/sdpa_attention.py:82: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  is_causal = query.shape[2] > 1 and attention_mask is None and getattr(module, \"is_causal\", True)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/onnx/_internal/torchscript_exporter/symbolic_opset9.py:5353: UserWarning: Exporting aten::index operator of advanced indexing in opset 18 is achieved by combination of multiple ONNX operators, including Reshape, Transpose, Concat, and Gather. If indices include negative values, the exported graph will produce incorrect results.\n",
            "  warnings.warn(\n",
            "Found different candidate ONNX initializers (likely duplicate) for the tied weights:\n",
            "\tlm_head.weight: {'onnx::MatMul_8030'}\n",
            "\tmodel.embed_tokens.weight: {'model.embed_tokens.weight'}\n",
            "\t\t-[x] values not close enough, max diff: 0.0006170272827148438 (atol: 1e-05)\n",
            "\t\t-[x] values not close enough, max diff: 3.600120544433594e-05 (atol: 1e-05)\n",
            "\t\t-[x] values not close enough, max diff: 1.430511474609375e-05 (atol: 1e-05)\n",
            "\t\t-[x] values not close enough, max diff: 6.198883056640625e-05 (atol: 1e-05)\n",
            "\t\t-[x] values not close enough, max diff: 2.136826515197754e-05 (atol: 1e-05)\n",
            "\t\t-[x] values not close enough, max diff: 8.392333984375e-05 (atol: 1e-05)\n",
            "\t\t-[x] values not close enough, max diff: 2.580881118774414e-05 (atol: 1e-05)\n",
            "\t\t-[x] values not close enough, max diff: 4.38690185546875e-05 (atol: 1e-05)\n",
            "\t\t-[x] values not close enough, max diff: 2.7835369110107422e-05 (atol: 1e-05)\n",
            "\t\t-[x] values not close enough, max diff: 5.4836273193359375e-05 (atol: 1e-05)\n",
            "\t\t-[x] values not close enough, max diff: 3.6366283893585205e-05 (atol: 1e-05)\n",
            "\t\t-[x] values not close enough, max diff: 6.723403930664062e-05 (atol: 1e-05)\n",
            "\t\t-[x] values not close enough, max diff: 3.266334533691406e-05 (atol: 1e-05)\n",
            "\t\t-[x] values not close enough, max diff: 4.8786401748657227e-05 (atol: 1e-05)\n",
            "\t\t-[x] values not close enough, max diff: 2.7447938919067383e-05 (atol: 1e-05)\n",
            "\t\t-[x] values not close enough, max diff: 5.555152893066406e-05 (atol: 1e-05)\n",
            "\t\t-[x] values not close enough, max diff: 2.6464462280273438e-05 (atol: 1e-05)\n",
            "\t\t-[x] values not close enough, max diff: 0.00010898709297180176 (atol: 1e-05)\n",
            "\t\t-[x] values not close enough, max diff: 3.331899642944336e-05 (atol: 1e-05)\n",
            "\t\t-[x] values not close enough, max diff: 9.733438491821289e-05 (atol: 1e-05)\n",
            "\t\t-[x] values not close enough, max diff: 5.0961971282958984e-05 (atol: 1e-05)\n",
            "\t\t-[x] values not close enough, max diff: 8.678436279296875e-05 (atol: 1e-05)\n",
            "\t\t-[x] values not close enough, max diff: 4.373490810394287e-05 (atol: 1e-05)\n",
            "\t\t-[x] values not close enough, max diff: 7.390975952148438e-05 (atol: 1e-05)\n",
            "\t\t-[x] values not close enough, max diff: 4.571676254272461e-05 (atol: 1e-05)\n",
            "\t\t-[x] values not close enough, max diff: 6.29425048828125e-05 (atol: 1e-05)\n",
            "\t\t-[x] values not close enough, max diff: 4.6372413635253906e-05 (atol: 1e-05)\n",
            "\t\t-[x] values not close enough, max diff: 0.00010395050048828125 (atol: 1e-05)\n",
            "\t\t-[x] values not close enough, max diff: 7.11679458618164e-05 (atol: 1e-05)\n",
            "\t\t-[x] values not close enough, max diff: 0.00010275840759277344 (atol: 1e-05)\n",
            "\t\t-[x] values not close enough, max diff: 5.435943603515625e-05 (atol: 1e-05)\n",
            "\t\t-[x] values not close enough, max diff: 7.462501525878906e-05 (atol: 1e-05)\n",
            "\t\t-[x] values not close enough, max diff: 3.898143768310547e-05 (atol: 1e-05)\n",
            "\t\t-[x] values not close enough, max diff: 9.295344352722168e-05 (atol: 1e-05)\n",
            "\t\t-[x] values not close enough, max diff: 4.6372413635253906e-05 (atol: 1e-05)\n",
            "\t\t-[x] values not close enough, max diff: 7.796287536621094e-05 (atol: 1e-05)\n",
            "\t\t-[x] values not close enough, max diff: 4.470348358154297e-05 (atol: 1e-05)\n",
            "\t\t-[x] values not close enough, max diff: 9.012222290039062e-05 (atol: 1e-05)\n",
            "\t\t-[x] values not close enough, max diff: 7.176399230957031e-05 (atol: 1e-05)\n",
            "\t\t-[x] values not close enough, max diff: 9.265542030334473e-05 (atol: 1e-05)\n",
            "\t\t-[x] values not close enough, max diff: 0.00011229515075683594 (atol: 1e-05)\n",
            "\t\t-[x] values not close enough, max diff: 8.20159912109375e-05 (atol: 1e-05)\n",
            "\t\t-[x] values not close enough, max diff: 0.000156402587890625 (atol: 1e-05)\n",
            "\t\t-[x] values not close enough, max diff: 0.00010025501251220703 (atol: 1e-05)\n",
            "\t\t-[x] values not close enough, max diff: 0.0001952648162841797 (atol: 1e-05)\n",
            "\t\t-[x] values not close enough, max diff: 0.00011801719665527344 (atol: 1e-05)\n",
            "\t\t-[x] values not close enough, max diff: 0.000244140625 (atol: 1e-05)\n",
            "The ONNX export succeeded with the warning: The maximum absolute difference between the output of the reference model and the ONNX exported model is not within the set tolerance 1e-05:\n",
            "- logits: max diff = 0.0006170272827148438\n",
            "- present.1.key: max diff = 3.600120544433594e-05\n",
            "- present.1.value: max diff = 1.430511474609375e-05\n",
            "- present.2.key: max diff = 6.198883056640625e-05\n",
            "- present.2.value: max diff = 2.136826515197754e-05\n",
            "- present.3.key: max diff = 8.392333984375e-05\n",
            "- present.3.value: max diff = 2.580881118774414e-05\n",
            "- present.4.key: max diff = 4.38690185546875e-05\n",
            "- present.4.value: max diff = 2.7835369110107422e-05\n",
            "- present.5.key: max diff = 5.4836273193359375e-05\n",
            "- present.5.value: max diff = 3.6366283893585205e-05\n",
            "- present.6.key: max diff = 6.723403930664062e-05\n",
            "- present.6.value: max diff = 3.266334533691406e-05\n",
            "- present.7.key: max diff = 4.8786401748657227e-05\n",
            "- present.7.value: max diff = 2.7447938919067383e-05\n",
            "- present.8.key: max diff = 5.555152893066406e-05\n",
            "- present.8.value: max diff = 2.6464462280273438e-05\n",
            "- present.9.key: max diff = 0.00010898709297180176\n",
            "- present.9.value: max diff = 3.331899642944336e-05\n",
            "- present.10.key: max diff = 9.733438491821289e-05\n",
            "- present.10.value: max diff = 5.0961971282958984e-05\n",
            "- present.11.key: max diff = 8.678436279296875e-05\n",
            "- present.11.value: max diff = 4.373490810394287e-05\n",
            "- present.12.key: max diff = 7.390975952148438e-05\n",
            "- present.12.value: max diff = 4.571676254272461e-05\n",
            "- present.13.key: max diff = 6.29425048828125e-05\n",
            "- present.13.value: max diff = 4.6372413635253906e-05\n",
            "- present.14.key: max diff = 0.00010395050048828125\n",
            "- present.14.value: max diff = 7.11679458618164e-05\n",
            "- present.15.key: max diff = 0.00010275840759277344\n",
            "- present.15.value: max diff = 5.435943603515625e-05\n",
            "- present.16.key: max diff = 7.462501525878906e-05\n",
            "- present.16.value: max diff = 3.898143768310547e-05\n",
            "- present.17.key: max diff = 9.295344352722168e-05\n",
            "- present.17.value: max diff = 4.6372413635253906e-05\n",
            "- present.18.key: max diff = 7.796287536621094e-05\n",
            "- present.18.value: max diff = 4.470348358154297e-05\n",
            "- present.19.key: max diff = 9.012222290039062e-05\n",
            "- present.19.value: max diff = 7.176399230957031e-05\n",
            "- present.20.key: max diff = 9.265542030334473e-05\n",
            "- present.20.value: max diff = 0.00011229515075683594\n",
            "- present.21.key: max diff = 8.20159912109375e-05\n",
            "- present.21.value: max diff = 0.000156402587890625\n",
            "- present.22.key: max diff = 0.00010025501251220703\n",
            "- present.22.value: max diff = 0.0001952648162841797\n",
            "- present.23.key: max diff = 0.00011801719665527344\n",
            "- present.23.value: max diff = 0.000244140625.\n",
            " The exported model was saved at: qwen2.5-0.5B-onnx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -alh qwen2.5-0.5B-onnx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O3yKxoDKLo_X",
        "outputId": "55907255-2d0b-487c-f8af-877bf20b491b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 1.9G\n",
            "drwxr-xr-x 2 root root  12K Dec 18 01:56 .\n",
            "drwxr-xr-x 1 root root 4.0K Dec 18 01:54 ..\n",
            "-rw-r--r-- 1 root root  605 Dec 18 01:54 added_tokens.json\n",
            "-rw-r--r-- 1 root root 2.5K Dec 18 01:54 chat_template.jinja\n",
            "-rw-r--r-- 1 root root 1.3K Dec 18 01:54 config.json\n",
            "-rw-r--r-- 1 root root  242 Dec 18 01:54 generation_config.json\n",
            "-rw-r--r-- 1 root root 1.6M Dec 18 01:54 merges.txt\n",
            "-rw-r--r-- 1 root root 1.1M Dec 18 01:56 model.onnx\n",
            "-rw-r--r-- 1 root root 1.9G Dec 18 01:56 model.onnx_data\n",
            "-rw-r--r-- 1 root root  613 Dec 18 01:54 special_tokens_map.json\n",
            "-rw-r--r-- 1 root root 4.6K Dec 18 01:54 tokenizer_config.json\n",
            "-rw-r--r-- 1 root root  11M Dec 18 01:54 tokenizer.json\n",
            "-rw-r--r-- 1 root root 2.7M Dec 18 01:54 vocab.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd local-llms-on-android/scripts && \\\n",
        "  python infer_onnx_with_kv_cache.py \\\n",
        "    --model qwen2.5 \\\n",
        "    --model_path /content/qwen2.5-0.5B-onnx/model.onnx \\\n",
        "    --tokenizer_path /content/qwen2.5-0.5B-onnx/tokenizer.json \\\n",
        "    --prompt \"What is the capital of Thailand?\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gy1aPrteL21c",
        "outputId": "742d1bed-e204-4828-ee19-f3c9866c2609"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Generated Text ===\n",
            " Bangkok\n",
            "\n",
            "The capital of Thailand is Bangkok. It is the largest city in Thailand and the country's economic and political center. The city is located on the Chao Phraya River and is known for its rich culture, history, and vibrant street\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tar zcvf qwen2.5-0.5B-onnx.tar.gz qwen2.5-0.5B-onnx"
      ],
      "metadata": {
        "id": "OR6_4tOdUpBp",
        "outputId": "cdbf3fef-dbd9-40a0-f5ba-d346265c5cf0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "qwen2.5-0.5B-onnx/\n",
            "qwen2.5-0.5B-onnx/merges.txt\n",
            "qwen2.5-0.5B-onnx/config.json\n",
            "qwen2.5-0.5B-onnx/vocab.json\n",
            "qwen2.5-0.5B-onnx/special_tokens_map.json\n",
            "qwen2.5-0.5B-onnx/chat_template.jinja\n",
            "qwen2.5-0.5B-onnx/added_tokens.json\n",
            "qwen2.5-0.5B-onnx/tokenizer.json\n",
            "qwen2.5-0.5B-onnx/tokenizer_config.json\n",
            "qwen2.5-0.5B-onnx/generation_config.json\n",
            "qwen2.5-0.5B-onnx/model.onnx_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tar zxvf /content/drive/MyDrive/<Path>/qwen2.5-0.5B-onnx.tar.gz"
      ],
      "metadata": {
        "id": "CDyJxTukh8oe",
        "outputId": "07c6c890-7384-4c77-82ce-413039487136",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "qwen2.5-0.5B-onnx/\n",
            "qwen2.5-0.5B-onnx/merges.txt\n",
            "qwen2.5-0.5B-onnx/config.json\n",
            "qwen2.5-0.5B-onnx/vocab.json\n",
            "qwen2.5-0.5B-onnx/special_tokens_map.json\n",
            "qwen2.5-0.5B-onnx/chat_template.jinja\n",
            "qwen2.5-0.5B-onnx/added_tokens.json\n",
            "qwen2.5-0.5B-onnx/tokenizer.json\n",
            "qwen2.5-0.5B-onnx/tokenizer_config.json\n",
            "qwen2.5-0.5B-onnx/generation_config.json\n",
            "qwen2.5-0.5B-onnx/model.onnx_data\n",
            "qwen2.5-0.5B-onnx/model.onnx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!optimum-cli export onnx \\\n",
        "  --model Qwen/Qwen3-0.6B qwen3-0.6B-onnx/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BWTuRvlxNa1Q",
        "outputId": "497c18d4-64df-48b1-cf43-ceb1e35c0d4c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: optimum-cli: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -alh qwen3-0.6B-onnx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "760eppvIP_y0",
        "outputId": "50f6ebb1-979e-4fa1-babf-2a018e72ee42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 2.3G\n",
            "drwxr-xr-x 2 root root  12K Dec 18 02:08 .\n",
            "drwxr-xr-x 1 root root 4.0K Dec 18 02:07 ..\n",
            "-rw-r--r-- 1 root root  707 Dec 18 02:07 added_tokens.json\n",
            "-rw-r--r-- 1 root root 4.1K Dec 18 02:07 chat_template.jinja\n",
            "-rw-r--r-- 1 root root 1.4K Dec 18 02:07 config.json\n",
            "-rw-r--r-- 1 root root  214 Dec 18 02:07 generation_config.json\n",
            "-rw-r--r-- 1 root root 1.6M Dec 18 02:07 merges.txt\n",
            "-rw-r--r-- 1 root root 1.4M Dec 18 02:08 model.onnx\n",
            "-rw-r--r-- 1 root root 2.3G Dec 18 02:08 model.onnx_data\n",
            "-rw-r--r-- 1 root root  613 Dec 18 02:07 special_tokens_map.json\n",
            "-rw-r--r-- 1 root root 5.3K Dec 18 02:07 tokenizer_config.json\n",
            "-rw-r--r-- 1 root root  11M Dec 18 02:07 tokenizer.json\n",
            "-rw-r--r-- 1 root root 2.7M Dec 18 02:07 vocab.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd local-llms-on-android/scripts && \\\n",
        "  python infer_onnx_with_kv_cache.py \\\n",
        "    --model qwen3 \\\n",
        "    --model_path /content/qwen3-0.6B-onnx/model.onnx \\\n",
        "    --tokenizer_path /content/qwen3-0.6B-onnx/tokenizer.json \\\n",
        "    --prompt \"What is the capital of Thailand?\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jc-YGNMNQE0K",
        "outputId": "56c0e133-93ba-4995-e24e-a4e075e5ca82"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Generated Text ===\n",
            " The capital of Thailand is Bangkok. But wait, I need to check if there are any other cities that might be considered the capital. For example, maybe there's a city that is more important in some way. Also, I should consider if there\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tar zcvf qwen3-0.6B-onnx.tar.gz qwen3-0.6B-onnx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yr-t4R6jQnEu",
        "outputId": "e613e07d-6661-481a-da95-b710d33d17d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "qwen3-0.6B-onnx/\n",
            "qwen3-0.6B-onnx/merges.txt\n",
            "qwen3-0.6B-onnx/config.json\n",
            "qwen3-0.6B-onnx/vocab.json\n",
            "qwen3-0.6B-onnx/special_tokens_map.json\n",
            "qwen3-0.6B-onnx/chat_template.jinja\n",
            "qwen3-0.6B-onnx/added_tokens.json\n",
            "qwen3-0.6B-onnx/tokenizer.json\n",
            "qwen3-0.6B-onnx/tokenizer_config.json\n",
            "qwen3-0.6B-onnx/generation_config.json\n",
            "qwen3-0.6B-onnx/model.onnx_data\n",
            "qwen3-0.6B-onnx/model.onnx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tar zxvf /content/drive/MyDrive/<Path>/qwen3-0.6B-onnx.tar.gz"
      ],
      "metadata": {
        "id": "0DdtCQTui85Q",
        "outputId": "042c17cd-0316-496e-8da8-f65a162d0993",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "qwen3-0.6B-onnx/\n",
            "qwen3-0.6B-onnx/merges.txt\n",
            "qwen3-0.6B-onnx/config.json\n",
            "qwen3-0.6B-onnx/vocab.json\n",
            "qwen3-0.6B-onnx/special_tokens_map.json\n",
            "qwen3-0.6B-onnx/chat_template.jinja\n",
            "qwen3-0.6B-onnx/added_tokens.json\n",
            "qwen3-0.6B-onnx/tokenizer.json\n",
            "qwen3-0.6B-onnx/tokenizer_config.json\n",
            "qwen3-0.6B-onnx/generation_config.json\n",
            "qwen3-0.6B-onnx/model.onnx_data\n",
            "qwen3-0.6B-onnx/model.onnx\n"
          ]
        }
      ]
    }
  ]
}