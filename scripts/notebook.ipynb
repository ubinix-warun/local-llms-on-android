{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPdHirUcMTjAoJmVm0oIjla",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ubinix-warun/local-llms-on-android/blob/main/scripts/notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ubinix-warun/local-llms-on-android.git"
      ],
      "metadata": {
        "id": "Uvt53A-N9O87",
        "outputId": "80246658-6ef4-4785-c2a3-2927af95d014",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'local-llms-on-android'...\n",
            "remote: Enumerating objects: 623, done.\u001b[K\n",
            "remote: Counting objects: 100% (381/381), done.\u001b[K\n",
            "remote: Compressing objects: 100% (243/243), done.\u001b[K\n",
            "remote: Total 623 (delta 168), reused 255 (delta 88), pack-reused 242 (from 1)\u001b[K\n",
            "Receiving objects: 100% (623/623), 24.91 MiB | 19.20 MiB/s, done.\n",
            "Resolving deltas: 100% (243/243), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd local-llms-on-android/scripts && \\\n",
        "  pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "8mijLs6w-QAJ",
        "outputId": "99cba438-9a33-4d8e-deeb-d102ae98ad1f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optimum@ git+https://github.com/huggingface/optimum.git (from -r requirements.txt (line 12))\n",
            "  Cloning https://github.com/huggingface/optimum.git to /tmp/pip-install-1p6th_z9/optimum_05b5828d55674ba683e63c3e7459fe4e\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/optimum.git /tmp/pip-install-1p6th_z9/optimum_05b5828d55674ba683e63c3e7459fe4e\n",
            "  Resolved https://github.com/huggingface/optimum.git to commit 0227a1ce9652b1b02da5a510bf513c585608f8c2\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting onnx (from -r requirements.txt (line 2))\n",
            "  Downloading onnx-1.20.0-cp312-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
            "Collecting onnxruntime (from -r requirements.txt (line 3))\n",
            "  Downloading onnxruntime-1.23.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\n",
            "Collecting onnxruntime-gpu (from -r requirements.txt (line 4))\n",
            "  Downloading onnxruntime_gpu-1.23.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.4 kB)\n",
            "Collecting onnx_graphsurgeon (from -r requirements.txt (line 5))\n",
            "  Downloading onnx_graphsurgeon-0.5.8-py2.py3-none-any.whl.metadata (8.2 kB)\n",
            "Collecting onnxconverter-common (from -r requirements.txt (line 6))\n",
            "  Downloading onnxconverter_common-1.16.0-py2.py3-none-any.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 7)) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 10)) (0.22.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 11)) (4.57.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 15)) (2.0.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 16)) (1.14.0)\n",
            "Requirement already satisfied: mpmath in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 17)) (1.3.0)\n",
            "Requirement already satisfied: protobuf>=4.25.1 in /usr/local/lib/python3.12/dist-packages (from onnx->-r requirements.txt (line 2)) (5.29.5)\n",
            "Requirement already satisfied: typing_extensions>=4.7.1 in /usr/local/lib/python3.12/dist-packages (from onnx->-r requirements.txt (line 2)) (4.15.0)\n",
            "Requirement already satisfied: ml_dtypes>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from onnx->-r requirements.txt (line 2)) (0.5.4)\n",
            "Collecting coloredlogs (from onnxruntime->-r requirements.txt (line 3))\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime->-r requirements.txt (line 3)) (25.9.23)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from onnxruntime->-r requirements.txt (line 3)) (25.0)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.16.4 in /usr/local/lib/python3.12/dist-packages (from tokenizers->-r requirements.txt (line 10)) (0.36.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers->-r requirements.txt (line 11)) (3.20.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers->-r requirements.txt (line 11)) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers->-r requirements.txt (line 11)) (2.32.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers->-r requirements.txt (line 11)) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers->-r requirements.txt (line 11)) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11 in /usr/local/lib/python3.12/dist-packages (from optimum@ git+https://github.com/huggingface/optimum.git->-r requirements.txt (line 12)) (2.9.0+cu126)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->-r requirements.txt (line 10)) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->-r requirements.txt (line 10)) (1.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->optimum@ git+https://github.com/huggingface/optimum.git->-r requirements.txt (line 12)) (75.2.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->optimum@ git+https://github.com/huggingface/optimum.git->-r requirements.txt (line 12)) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->optimum@ git+https://github.com/huggingface/optimum.git->-r requirements.txt (line 12)) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->optimum@ git+https://github.com/huggingface/optimum.git->-r requirements.txt (line 12)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->optimum@ git+https://github.com/huggingface/optimum.git->-r requirements.txt (line 12)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->optimum@ git+https://github.com/huggingface/optimum.git->-r requirements.txt (line 12)) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->optimum@ git+https://github.com/huggingface/optimum.git->-r requirements.txt (line 12)) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->optimum@ git+https://github.com/huggingface/optimum.git->-r requirements.txt (line 12)) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->optimum@ git+https://github.com/huggingface/optimum.git->-r requirements.txt (line 12)) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->optimum@ git+https://github.com/huggingface/optimum.git->-r requirements.txt (line 12)) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->optimum@ git+https://github.com/huggingface/optimum.git->-r requirements.txt (line 12)) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->optimum@ git+https://github.com/huggingface/optimum.git->-r requirements.txt (line 12)) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->optimum@ git+https://github.com/huggingface/optimum.git->-r requirements.txt (line 12)) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->optimum@ git+https://github.com/huggingface/optimum.git->-r requirements.txt (line 12)) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->optimum@ git+https://github.com/huggingface/optimum.git->-r requirements.txt (line 12)) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->optimum@ git+https://github.com/huggingface/optimum.git->-r requirements.txt (line 12)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->optimum@ git+https://github.com/huggingface/optimum.git->-r requirements.txt (line 12)) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->optimum@ git+https://github.com/huggingface/optimum.git->-r requirements.txt (line 12)) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->optimum@ git+https://github.com/huggingface/optimum.git->-r requirements.txt (line 12)) (3.5.0)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime->-r requirements.txt (line 3))\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers->-r requirements.txt (line 11)) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers->-r requirements.txt (line 11)) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers->-r requirements.txt (line 11)) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers->-r requirements.txt (line 11)) (2025.11.12)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11->optimum@ git+https://github.com/huggingface/optimum.git->-r requirements.txt (line 12)) (3.0.3)\n",
            "Downloading onnx-1.20.0-cp312-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (18.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.1/18.1 MB\u001b[0m \u001b[31m97.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.23.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (17.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m98.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime_gpu-1.23.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (300.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.5/300.5 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnx_graphsurgeon-0.5.8-py2.py3-none-any.whl (57 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.9/57.9 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxconverter_common-1.16.0-py2.py3-none-any.whl (89 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.5/89.5 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: optimum\n",
            "  Building wheel for optimum (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for optimum: filename=optimum-2.1.0.dev0-py3-none-any.whl size=162229 sha256=a54056fec921a37eae8306fa10830a48fd25bebec8582c27c0ff4dbdb00ceffc\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-g7ueec1j/wheels/88/3d/4e/082f1a517240fa457dbd77485e6d08a64f31cbcc0ca7526605\n",
            "Successfully built optimum\n",
            "Installing collected packages: humanfriendly, onnx, coloredlogs, onnxruntime-gpu, onnxruntime, onnxconverter-common, onnx_graphsurgeon, optimum\n",
            "Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 onnx-1.20.0 onnx_graphsurgeon-0.5.8 onnxconverter-common-1.16.0 onnxruntime-1.23.2 onnxruntime-gpu-1.23.2 optimum-2.1.0.dev0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://huggingface.co/onnx-community/Qwen3-0.6B-ONNX/resolve/main/onnx/model_q4.onnx"
      ],
      "metadata": {
        "id": "YGxi4JEt95xh",
        "outputId": "a8af4857-0ca2-4d66-9174-56832d322eac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-12-09 12:01:38--  https://huggingface.co/onnx-community/Qwen3-0.6B-ONNX/resolve/main/onnx/model_q4.onnx\n",
            "Resolving huggingface.co (huggingface.co)... 18.164.174.118, 18.164.174.17, 18.164.174.23, ...\n",
            "Connecting to huggingface.co (huggingface.co)|18.164.174.118|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cas-bridge.xethub.hf.co/xet-bridge-us/680fffcdbe536d99d3cb19b9/c5e1a7c527e70efb93d47c9058c3bb2b5a3cc56700a07479f55aa4c1b9981a7c?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20251209%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20251209T120139Z&X-Amz-Expires=3600&X-Amz-Signature=6ac6b823cb59c6cb5355a6ab71a0db95be40aa94f54ee8495179c9c48f2d5814&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27model_q4.onnx%3B+filename%3D%22model_q4.onnx%22%3B&x-id=GetObject&Expires=1765285299&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc2NTI4NTI5OX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82ODBmZmZjZGJlNTM2ZDk5ZDNjYjE5YjkvYzVlMWE3YzUyN2U3MGVmYjkzZDQ3YzkwNThjM2JiMmI1YTNjYzU2NzAwYTA3NDc5ZjU1YWE0YzFiOTk4MWE3YyoifV19&Signature=jVdvoUsgnJiJ2kW7587Rtk%7EEtmSIzVCgD21OVS4NjD8jHIwxj%7EtNIPtIP4ZNNODRGy3PFHsKthd%7E45gqdoMtV-ghGegmGyAEsYQ7xUAxTjsX72yILHOZUHQygb6B04R0nwHNbZlnZvAKAf1aeZMxSVA-W35d0Yqh6351HvbGZWm4z9cGDii1fgQWt813dIZGSwTXngrJArEXRGwsYDmZch9clgwLw-FjeF-Uc-4WZEckxrrGSvLh7l-prQR0kwQZV7my3cEDO2YzmqPgE0Rl-ODTE5URLdQH6p1GhqcmaXo-dZdbkfkc4Cdg1zO1Mwd%7E%7E2ReWAfmYFYVGIdurYiyXw__&Key-Pair-Id=K2L8F4GPSG1IFC [following]\n",
            "--2025-12-09 12:01:39--  https://cas-bridge.xethub.hf.co/xet-bridge-us/680fffcdbe536d99d3cb19b9/c5e1a7c527e70efb93d47c9058c3bb2b5a3cc56700a07479f55aa4c1b9981a7c?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20251209%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20251209T120139Z&X-Amz-Expires=3600&X-Amz-Signature=6ac6b823cb59c6cb5355a6ab71a0db95be40aa94f54ee8495179c9c48f2d5814&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27model_q4.onnx%3B+filename%3D%22model_q4.onnx%22%3B&x-id=GetObject&Expires=1765285299&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc2NTI4NTI5OX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82ODBmZmZjZGJlNTM2ZDk5ZDNjYjE5YjkvYzVlMWE3YzUyN2U3MGVmYjkzZDQ3YzkwNThjM2JiMmI1YTNjYzU2NzAwYTA3NDc5ZjU1YWE0YzFiOTk4MWE3YyoifV19&Signature=jVdvoUsgnJiJ2kW7587Rtk%7EEtmSIzVCgD21OVS4NjD8jHIwxj%7EtNIPtIP4ZNNODRGy3PFHsKthd%7E45gqdoMtV-ghGegmGyAEsYQ7xUAxTjsX72yILHOZUHQygb6B04R0nwHNbZlnZvAKAf1aeZMxSVA-W35d0Yqh6351HvbGZWm4z9cGDii1fgQWt813dIZGSwTXngrJArEXRGwsYDmZch9clgwLw-FjeF-Uc-4WZEckxrrGSvLh7l-prQR0kwQZV7my3cEDO2YzmqPgE0Rl-ODTE5URLdQH6p1GhqcmaXo-dZdbkfkc4Cdg1zO1Mwd%7E%7E2ReWAfmYFYVGIdurYiyXw__&Key-Pair-Id=K2L8F4GPSG1IFC\n",
            "Resolving cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)... 18.164.174.21, 18.164.174.4, 18.164.174.110, ...\n",
            "Connecting to cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)|18.164.174.21|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 919096585 (877M)\n",
            "Saving to: ‘model_q4.onnx’\n",
            "\n",
            "model_q4.onnx       100%[===================>] 876.52M  77.0MB/s    in 12s     \n",
            "\n",
            "2025-12-09 12:01:51 (74.5 MB/s) - ‘model_q4.onnx’ saved [919096585/919096585]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://huggingface.co/onnx-community/Qwen3-0.6B-ONNX/resolve/main/tokenizer.json"
      ],
      "metadata": {
        "id": "mps73Q3F_z1Z",
        "outputId": "4c4db30e-4c1a-4c9c-b725-25386d025616",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-12-09 12:05:50--  https://huggingface.co/onnx-community/Qwen3-0.6B-ONNX/resolve/main/tokenizer.json\n",
            "Resolving huggingface.co (huggingface.co)... 18.164.174.118, 18.164.174.23, 18.164.174.17, ...\n",
            "Connecting to huggingface.co (huggingface.co)|18.164.174.118|:443... connected.\n",
            "HTTP request sent, awaiting response... 307 Temporary Redirect\n",
            "Location: /api/resolve-cache/models/onnx-community/Qwen3-0.6B-ONNX/b1ece21c06dfce3839272e86b7fa12a985d97a7a/tokenizer.json?%2Fonnx-community%2FQwen3-0.6B-ONNX%2Fresolve%2Fmain%2Ftokenizer.json=&etag=%226c0f4b0f97c638fb9b293aaa8bf99a159e650ed0%22 [following]\n",
            "--2025-12-09 12:05:50--  https://huggingface.co/api/resolve-cache/models/onnx-community/Qwen3-0.6B-ONNX/b1ece21c06dfce3839272e86b7fa12a985d97a7a/tokenizer.json?%2Fonnx-community%2FQwen3-0.6B-ONNX%2Fresolve%2Fmain%2Ftokenizer.json=&etag=%226c0f4b0f97c638fb9b293aaa8bf99a159e650ed0%22\n",
            "Reusing existing connection to huggingface.co:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9117040 (8.7M) [text/plain]\n",
            "Saving to: ‘tokenizer.json’\n",
            "\n",
            "tokenizer.json      100%[===================>]   8.69M  --.-KB/s    in 0.07s   \n",
            "\n",
            "2025-12-09 12:05:50 (123 MB/s) - ‘tokenizer.json’ saved [9117040/9117040]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "S7rUDsV67uXr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9a71809-2ca6-4cfe-f551-d92c46f3f03d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Generated Text ===\n",
            " What is the capital of France? What is the capital of Italy? What is the capital of Spain? What is the capital of Russia? What is the capital of Brazil? What is the capital of China? What is the capital of Japan? What\n"
          ]
        }
      ],
      "source": [
        "!cd local-llms-on-android/scripts && \\\n",
        "  python infer_onnx_with_kv_cache.py \\\n",
        "    --model qwen3 \\\n",
        "    --model_path /content/model_q4.onnx \\\n",
        "    --tokenizer_path /content/tokenizer.json \\\n",
        "    --prompt \"What is the capital of Germany?\""
      ]
    }
  ]
}